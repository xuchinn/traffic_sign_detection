{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT STATEMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import cv2\n",
    "import pickle\n",
    "import os as os\n",
    "import tensorflow as tf\n",
    "from torchvision import transforms\n",
    "import time\n",
    "import math\n",
    "import numpy as np\n",
    "from settings import *\n",
    "from data_prep import calc_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"D:/Semester 6/Deep Learning/uas_dataset/\"\n",
    "train_path = 'output/obj'\n",
    "test_path = 'test'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTIONS TO LOAD IMAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_path):\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # Convert BGR to RGB\n",
    "    return image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FUNCTION TO READ .TXT FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_annotation_from_txt(annotation_path):\n",
    "    with open(annotation_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    annotation = []\n",
    "    for line in lines:\n",
    "        line = line.strip().split(' ')\n",
    "        class_label, xmin, ymin, xmax, ymax= map(float, line)\n",
    "        annotation.append([class_label, xmin, ymin, xmax, ymax])\n",
    "\n",
    "    return annotation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LOAD CUSTOM DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, image_paths, annotation_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.annotation_paths = annotation_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Load the image\n",
    "        image = load_image(self.image_paths[index])\n",
    "\n",
    "        # Load and parse the annotation from the .txt file\n",
    "        annotation = load_annotation_from_txt(self.annotation_paths[index])\n",
    "\n",
    "        # Preprocess the image and annotation\n",
    "        if self.transform is not None:\n",
    "            image, annotation = self.transform(image, annotation)\n",
    "\n",
    "        return image, annotation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GET ANNOTATION PATH TRAIN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_txt_files = []\n",
    "file_list = os.listdir(path + train_path)\n",
    "for folder in file_list:\n",
    "    folder_path = os.path.join(path, train_path, folder)\n",
    "    folder_path = folder_path.replace('\\\\', '/')\n",
    "    if os.path.isdir(folder_path):\n",
    "        # Get a list of all files in the current folder\n",
    "        folder_files = os.listdir(folder_path)\n",
    "        # print(folder_files)\n",
    "        \n",
    "        # Filter out non-image files based on file extensions\n",
    "        folder_text_files = [file for file in folder_files if file.lower().endswith((\".txt\"))]\n",
    "\n",
    "        # Append the image files of the current folder to the overall image_files list\n",
    "        for file in folder_text_files:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            file_path = file_path.replace('\\\\', '/')\n",
    "            train_txt_files.extend([file_path])\n",
    "        \n",
    "# print(train_txt_files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GET ANNOTATION PATH TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_txt_files = []\n",
    "file_list = os.listdir(path + test_path)\n",
    "for folder in file_list:\n",
    "    folder_path = os.path.join(path, test_path, folder)\n",
    "    folder_path = folder_path.replace('\\\\', '/')\n",
    "    if os.path.isdir(folder_path):\n",
    "        # Get a list of all files in the current folder\n",
    "        folder_files = os.listdir(folder_path)\n",
    "        # print(folder_files)\n",
    "        \n",
    "        # Filter out non-image files based on file extensions\n",
    "        folder_text_files = [file for file in folder_files if file.lower().endswith((\".txt\"))]\n",
    "\n",
    "        # Append the image files of the current folder to the overall image_files list\n",
    "        for file in folder_text_files:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            file_path = file_path.replace('\\\\', '/')\n",
    "            test_txt_files.extend([file_path])\n",
    "        \n",
    "\n",
    "# print(test_txt_files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GET IMAGE PATH TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_image_files = []\n",
    "file_list = os.listdir(path + train_path)\n",
    "for folder in file_list:\n",
    "    folder_path = os.path.join(path, train_path, folder)\n",
    "    folder_path = folder_path.replace('\\\\', '/')\n",
    "    if os.path.isdir(folder_path):\n",
    "        # Get a list of all files in the current folder\n",
    "        folder_files = os.listdir(folder_path)\n",
    "        # print(folder_files)\n",
    "        \n",
    "        # Filter out non-image files based on file extensions\n",
    "        folder_image_files = [file for file in folder_files if file.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "        # Append the image files of the current folder to the overall image_files list\n",
    "        for file in folder_image_files:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            file_path = file_path.replace('\\\\', '/')\n",
    "            train_image_files.extend([file_path])   \n",
    "\n",
    "# print(train_image_files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GET IMAGE PATH TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_files = []\n",
    "file_list = os.listdir(path + test_path)\n",
    "for folder in file_list:\n",
    "    folder_path = os.path.join(path, test_path, folder)\n",
    "    folder_path = folder_path.replace('\\\\', '/')\n",
    "    if os.path.isdir(folder_path):\n",
    "        # Get a list of all files in the current folder\n",
    "        folder_files = os.listdir(folder_path)\n",
    "        # print(folder_files)\n",
    "        \n",
    "        # Filter out non-image files based on file extensions\n",
    "        folder_image_files = [file for file in folder_files if file.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "        # Append the image files of the current folder to the overall image_files list\n",
    "        for file in folder_image_files:\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            file_path = file_path.replace('\\\\', '/')\n",
    "            test_image_files.extend([file_path])\n",
    "\n",
    "# print(test_image_files)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DECLARE CUSTOM DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CustomDataset(train_image_files, train_txt_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torchvision.models as models\n",
    "\n",
    "# class SSDModel(nn.Module):\n",
    "#     def __init__(self, num_classes):\n",
    "#         super(SSDModel, self).__init__()\n",
    "\n",
    "#         # Load the pretrained base model\n",
    "#         self.base_model = models.vgg16(pretrained=True)\n",
    "\n",
    "#         # Modify the backbone\n",
    "#         # ...\n",
    "\n",
    "#         # Define default box aspect ratios and scales\n",
    "#         # ...\n",
    "\n",
    "#         # Define additional layers for prediction\n",
    "#         # ...\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Pass the input through the modified backbone\n",
    "#         features = self.base_model(x)\n",
    "\n",
    "#         # Generate anchor boxes\n",
    "#         # ...\n",
    "\n",
    "#         # Perform prediction on each feature map level\n",
    "#         # ...\n",
    "\n",
    "#         # Combine the predictions from different levels\n",
    "#         # ...\n",
    "\n",
    "#         return predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SSD HOOK FUNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSDHook(feature_map, layer_name):\n",
    "    \"\"\"\n",
    "    SSD hook for generating classification and localization predictions from a feature map\n",
    "    Args:\n",
    "        feature_map: Tensor, feature map from a layer in the network\n",
    "        layer_name: str, name of the layer\n",
    "    Returns:\n",
    "        net_conf: Tensor, classification predictions\n",
    "        net_loc: Tensor, localization predictions\n",
    "    \"\"\"\n",
    "    with tf.name_scope(layer_name):\n",
    "        # Note we have linear activation (i.e. no activation function)\n",
    "        net_conf = tf.keras.layers.Conv2D(NUM_PRED_CONF, [3, 3], activation=None, name='conv_conf')(feature_map)\n",
    "        net_conf = tf.keras.layers.Flatten()(net_conf)\n",
    "\n",
    "        net_loc = tf.keras.layers.Conv2D(NUM_PRED_LOC, [3, 3], activation=None, name='conv_loc')(feature_map)\n",
    "        net_loc = tf.keras.layers.Flatten()(net_loc)\n",
    "\n",
    "    return net_conf, net_loc"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MODEL HELPER FUNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ModelHelper(y_pred_conf, y_pred_loc):\n",
    "    \"\"\"\n",
    "\tDefine loss function, optimizer, predictions, and accuracy metric\n",
    "\tLoss includes confidence loss and localization loss\n",
    "\tconf_loss_mask is created at batch generation time, to mask the confidence losses\n",
    "\tIt has 1 at locations w/ positives, and 1 at select negative locations\n",
    "\tsuch that negative-to-positive ratio of NEG_POS_RATIO is satisfied\n",
    "\tArguments:\n",
    "\t\t* y_pred_conf: Class predictions from model,\n",
    "\t\t\ta tensor of shape [batch_size, num_feature_map_cells * num_default_boxes * num_classes]\n",
    "\t\t* y_pred_loc: Localization predictions from model,\n",
    "\t\t\ta tensor of shape [batch_size, num_feature_map_cells * num_default_boxes * 4]\n",
    "\tReturns relevant tensor references\n",
    "\t\"\"\"\n",
    "    num_total_preds = 0\n",
    "    for fm_size in FM_SIZES:\n",
    "        num_total_preds += fm_size[0] * fm_size[1] * NUM_DEFAULT_BOXES\n",
    "        # print(num_total_preds)\n",
    "    num_total_preds_conf = num_total_preds * NUM_CLASSES\n",
    "    num_total_preds_loc = num_total_preds * 4\n",
    "    print(num_total_preds_loc)\n",
    "    print(y_pred_loc.shape)\n",
    "\n",
    "    # Input tensors\n",
    "    y_true_conf = tf.keras.Input(shape=(num_total_preds,), dtype=tf.int32, name='y_true_conf')\n",
    "    y_true_loc = tf.keras.Input(shape=(num_total_preds_loc,), dtype=tf.float32, name='y_true_loc')\n",
    "    conf_loss_mask = tf.keras.Input(shape=(num_total_preds,), dtype=tf.float32, name='conf_loss_mask')\n",
    "\n",
    "    \n",
    "    # Confidence loss\n",
    "    logits = tf.reshape(y_pred_conf, [-1, num_total_preds, NUM_CLASSES])\n",
    "    conf_loss = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits, labels=y_true_conf)\n",
    "    conf_loss = conf_loss_mask * conf_loss\n",
    "    conf_loss = tf.reduce_sum(conf_loss)\n",
    "    \n",
    "    # Localization loss (smooth L1 loss)\n",
    "    diff = y_true_loc - y_pred_loc\n",
    "\n",
    "    loc_loss_l2 = 0.5 * (diff ** 2.0)\n",
    "    loc_loss_l1 = tf.abs(diff) - 0.5\n",
    "    smooth_l1_condition = tf.less(tf.abs(diff), 1.0)\n",
    "    loc_loss = tf.where(smooth_l1_condition, loc_loss_l2, loc_loss_l1)\n",
    "\n",
    "    loc_loss_mask = tf.minimum(y_true_conf, 1)\n",
    "    loc_loss_mask = tf.cast(loc_loss_mask, tf.float32)\n",
    "    loc_loss_mask = tf.reshape(loc_loss_mask, [-1, num_total_preds_loc])\n",
    "    loc_loss = loc_loss_mask * loc_loss\n",
    "    loc_loss = tf.reduce_sum(loc_loss)\n",
    "\n",
    "    # Weighted average of confidence loss and localization loss\n",
    "    # Also add regularization loss\n",
    "    loss = conf_loss + LOC_LOSS_WEIGHT * loc_loss + tf.reduce_sum(tf.losses.get_regularization_losses())\n",
    "    optimizer = OPT.minimize(loss)\n",
    "\n",
    "    probs_all = tf.nn.softmax(logits)\n",
    "    probs, preds_conf = tf.nn.top_k(probs_all)\n",
    "    probs = tf.reshape(probs, [-1, num_total_preds])\n",
    "    preds_conf = tf.reshape(preds_conf, [-1, num_total_preds])\n",
    "\n",
    "    ret_dict = {\n",
    "        'y_true_conf': y_true_conf,\n",
    "        'y_true_loc': y_true_loc,\n",
    "        'conf_loss_mask': conf_loss_mask,\n",
    "        'optimizer': optimizer,\n",
    "        'conf_loss': conf_loss,\n",
    "        'loc_loss': loc_loss,\n",
    "        'loss': loss,\n",
    "        'probs': probs,\n",
    "        'preds_conf': preds_conf,\n",
    "        'preds_loc': y_pred_loc,\n",
    "    }\n",
    "    return ret_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ALEX NET FUNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AlexNet():\n",
    "    \"\"\"\n",
    "    AlexNet\n",
    "    \"\"\"\n",
    "    # Image batch tensor and dropout keep prob placeholders\n",
    "    x = tf.keras.Input(shape=(IMG_H, IMG_W, NUM_CHANNELS), name='x')\n",
    "    is_training = tf.keras.Input(shape=(), dtype=tf.bool, name='is_training')\n",
    "\n",
    "    # Classification and localization predictions\n",
    "    preds_conf = []  # conf -> classification b/c confidence loss -> classification loss\n",
    "    preds_loc = []\n",
    "\n",
    "    # Use batch normalization for all convolution layers\n",
    "    net = tf.keras.layers.BatchNormalization()(x)\n",
    "    net = tf.keras.layers.Conv2D(64, [3, 3], activation=tf.nn.relu, padding='same', name='conv1_1')(net)\n",
    "    net = tf.keras.layers.Conv2D(64, [3, 3], activation=tf.nn.relu, padding='same', name='conv1_2')(net)\n",
    "    net = tf.keras.layers.MaxPooling2D([2, 2], strides=[2, 2], name='pool1')(net)\n",
    "\n",
    "    net = tf.keras.layers.BatchNormalization()(net)\n",
    "    net = tf.keras.layers.Conv2D(128, [3, 3], activation=tf.nn.relu, padding='same', name='conv2_1')(net)\n",
    "    net = tf.keras.layers.Conv2D(128, [3, 3], activation=tf.nn.relu, padding='same', name='conv2_2')(net)\n",
    "    net = tf.keras.layers.MaxPooling2D([2, 2], strides=[2, 2], name='pool2')(net)\n",
    "\n",
    "    net = tf.keras.layers.BatchNormalization()(net)\n",
    "    net = tf.keras.layers.Conv2D(256, [3, 3], activation=tf.nn.relu, padding='same', name='conv3_1')(net)\n",
    "    net = tf.keras.layers.Conv2D(256, [3, 3], activation=tf.nn.relu, padding='same', name='conv3_2')(net)\n",
    "    net = tf.keras.layers.Conv2D(256, [3, 3], activation=tf.nn.relu, padding='same', name='conv3_3')(net)\n",
    "    net = tf.keras.layers.MaxPooling2D([2, 2], strides=[2, 2], name='pool3')(net)\n",
    "\n",
    "    net = tf.keras.layers.BatchNormalization()(net)\n",
    "    net = tf.keras.layers.Conv2D(512, [3, 3], activation=tf.nn.relu, padding='same', name='conv4_1')(net)\n",
    "    net = tf.keras.layers.Conv2D(512, [3, 3], activation=tf.nn.relu, padding='same', name='conv4_2')(net)\n",
    "    net = tf.keras.layers.Conv2D(512, [3, 3], activation=tf.nn.relu, padding='same', name='conv4_3')(net)\n",
    "    net = tf.keras.layers.MaxPooling2D([2, 2], strides=[2, 2], name='pool4')(net)\n",
    "\n",
    "    net = tf.keras.layers.BatchNormalization()(net)\n",
    "    net = tf.keras.layers.Conv2D(512, [3, 3], activation=tf.nn.relu, padding='same', name='conv5_1')(net)\n",
    "    net = tf.keras.layers.Conv2D(512, [3, 3], activation=tf.nn.relu, padding='same', name='conv5_2')(net)\n",
    "    net = tf.keras.layers.Conv2D(512, [3, 3], activation=tf.nn.relu, padding='same', name='conv5_3')(net)\n",
    "\n",
    "    # The following layers added for SSD\n",
    "    net = tf.keras.layers.Conv2D(1024, [3, 3], activation=tf.nn.relu, padding='same', name='conv6')(net)\n",
    "    net = tf.keras.layers.Conv2D(1024, [1, 1], activation=tf.nn.relu, padding='same', name='conv7')(net)\n",
    "\n",
    "    net_conf, net_loc = SSDHook(net, 'conv7')\n",
    "    print(net_loc)\n",
    "    preds_conf.append(net_conf)\n",
    "    preds_loc.append(net_loc)\n",
    "\n",
    "    net = tf.keras.layers.Conv2D(256, [1, 1], activation=tf.nn.relu, padding='same', name='conv8')(net)\n",
    "    net = tf.keras.layers.Conv2D(512, [3, 3], activation=tf.nn.relu, padding='same', name='conv8_2')(net)\n",
    "\n",
    "    net = tf.keras.layers.Conv2D(256, [1, 1], activation=tf.nn.relu, padding='same', name='conv8')(net)\n",
    "    net = tf.keras.layers.Conv2D(512, [3, 3], strides=[2, 2], activation=tf.nn.relu, padding='same', name='conv8_2')(net)\n",
    "\n",
    "    net_conf, net_loc = SSDHook(net, 'conv8_2')\n",
    "    print(net_loc)\n",
    "    preds_conf.append(net_conf)\n",
    "    preds_loc.append(net_loc)\n",
    "\n",
    "    net = tf.keras.layers.Conv2D(128, [1, 1], activation=tf.nn.relu, padding='same', name='conv9')(net)\n",
    "    net = tf.keras.layers.Conv2D(256, [3, 3], strides=[2, 2], activation=tf.nn.relu, padding='same', name='conv9_2')(net)\n",
    "\n",
    "    net_conf, net_loc = SSDHook(net, 'conv9_2')\n",
    "    print(net_loc)\n",
    "    preds_conf.append(net_conf)\n",
    "    preds_loc.append(net_loc)\n",
    "\n",
    "    # Concatenate all preds together into 1 vector, for both classification and localization predictions\n",
    "    final_pred_conf = tf.keras.layers.Concatenate(axis=1)(preds_conf)\n",
    "    final_pred_loc = tf.keras.layers.Concatenate(axis=1)(preds_loc)\n",
    "    print(final_pred_loc)\n",
    "\n",
    "    # Return a dictionary of {tensor_name: tensor_reference}\n",
    "    ret_dict = {\n",
    "        'x': x,\n",
    "        'y_pred_conf': final_pred_conf,\n",
    "        'y_pred_loc': final_pred_loc,\n",
    "        'is_training': is_training\n",
    "    }\n",
    "\n",
    "    return ret_dict"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SSD MODEL FUNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SSDModel():\n",
    "    \"\"\"\n",
    "\tWrapper around the model and model helper\n",
    "\tReturns dict of relevant tensor references\n",
    "\t\"\"\"\n",
    "    if MODEL == 'AlexNet':\n",
    "        model = AlexNet()\n",
    "    else:\n",
    "        raise NotImplementedError('Model %s not supported' % MODEL)\n",
    "\n",
    "    model_helper = ModelHelper(model['y_pred_conf'], model['y_pred_loc'])\n",
    "\n",
    "    ssd_model = {}\n",
    "    for k in model.keys():\n",
    "        ssd_model[k] = model[k]\n",
    "    for k in model_helper.keys():\n",
    "        ssd_model[k] = model_helper[k]\n",
    "\n",
    "    return ssd_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMS (non maximum supression) FUNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nms(y_pred_conf, y_pred_loc, prob):\n",
    "    class_boxes = {}\n",
    "    for cls in range(NUM_CLASSES):\n",
    "        class_boxes[cls] = []\n",
    "\n",
    "    y_idx = 0\n",
    "    for fm_size in FM_SIZES:\n",
    "        fm_h, fm_w = fm_size\n",
    "        for row in range(fm_h):\n",
    "            for col in range(fm_w):\n",
    "                for db in range(NUM_DEFAULT_BOXES):\n",
    "                    if prob[y_idx] > CONF_THRESH and y_pred_conf[y_idx] > 0.:\n",
    "                        xc, yc = col + 0.5, row + 0.5\n",
    "                        center_coords = np.array([xc, yc, xc, yc])\n",
    "                        abs_box_coords = center_coords + y_pred_loc[y_idx * 4 : y_idx * 4 + 4]\n",
    "\n",
    "                        scale = np.array([IMG_W / fm_w, IMG_H / fm_h, IMG_W / fm_w, IMG_H / fm_h])\n",
    "                        box_coords = abs_box_coords * scale\n",
    "                        box_coords = [int(round(x)) for x in box_coords]\n",
    "\n",
    "                        cls = y_pred_conf[y_idx]\n",
    "                        cls_prob = prob[y_idx]\n",
    "                        box = (*box_coords, cls, cls_prob)\n",
    "                        if len(class_boxes[cls]) == 0:\n",
    "                            class_boxes[cls].append(box)\n",
    "                        else:\n",
    "                            suppressed = False\n",
    "                            overlapped = False\n",
    "                            for other_box in class_boxes[cls]:\n",
    "                                iou = calc_iou(box[:4], other_box[:4])\n",
    "                                if iou > NMS_IOU_THRESH:\n",
    "                                    overlapped = True\n",
    "                                    if box[5] > other_box[5]:\n",
    "                                        class_boxes[cls].remove(other_box)\n",
    "                                        suppressed = True\n",
    "                            if suppressed or not overlapped:\n",
    "                                class_boxes[cls].append(box)\n",
    "\n",
    "                    y_idx += 1\n",
    "\n",
    "    boxes = []\n",
    "    for cls in class_boxes.keys():\n",
    "        for class_box in class_boxes[cls]:\n",
    "            boxes.append(class_box)\n",
    "    boxes = np.array(boxes)\n",
    "\n",
    "    return boxes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEXT BATCH FUNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_batch(X, y_conf, y_loc, batch_size):\n",
    "\t\"\"\"\n",
    "\tNext batch generator\n",
    "\tArguments:\n",
    "\t\t* X: List of image file names\n",
    "\t\t* y_conf: List of ground-truth vectors for class labels\n",
    "\t\t* y_loc: List of ground-truth vectors for localization\n",
    "\t\t* batch_size: Batch size\n",
    "\n",
    "\tYields:\n",
    "\t\t* images: Batch numpy array representation of batch of images\n",
    "\t\t* y_true_conf: Batch numpy array of ground-truth class labels\n",
    "\t\t* y_true_loc: Batch numpy array of ground-truth localization\n",
    "\t\t* conf_loss_mask: Loss mask for confidence loss, to set NEG_POS_RATIO\n",
    "\t\"\"\"\n",
    "\ttransform = transforms.Compose([\n",
    "\t\ttransforms.Resize((IMG_W, IMG_H)),\n",
    "\t\ttransforms.ToTensor(),\n",
    "\t# Add other transformations as needed\n",
    "\t])\n",
    "\n",
    "\ttrain_dataset = CustomDataset(X, y_conf, y_loc, transform=transform)\n",
    "\ttrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\tfor images, y_true_conf, y_true_loc in train_dataloader:\n",
    "\t\t# Grayscale images have shape (batch_size, H, W), but we want shape (batch_size, H, W, 1)\n",
    "\t\tif NUM_CHANNELS == 1:\n",
    "\t\t\timages = images.unsqueeze(3)\n",
    "\t\telif NUM_CHANNELS == 3:\n",
    "\t\t\timages = images.unsqueeze(3).expand(-1, -1, -1, 3)\n",
    "\n",
    "\t\t# Normalize pixel values (scale them between -1 and 1)\n",
    "\t\timages = images / 127.5 - 1.\n",
    "\n",
    "\t\t# For y_true_conf, calculate how many negative examples we need to satisfy NEG_POS_RATIO\n",
    "\t\tnum_pos = torch.sum(y_true_conf > 0)\n",
    "\t\tnum_neg = NEG_POS_RATIO * num_pos\n",
    "\t\ty_true_conf_size = y_true_conf.numel()\n",
    "\n",
    "\t\t# Create confidence loss mask to satisfy NEG_POS_RATIO\n",
    "\t\tif num_pos + num_neg < y_true_conf_size:\n",
    "\t\t\tconf_loss_mask = y_true_conf.clone()\n",
    "\t\t\tconf_loss_mask[y_true_conf == 1] = 1\n",
    "\t\t\tnum_neg_needed = y_true_conf_size - num_pos\n",
    "\t\t\tneg_indices = torch.where(y_true_conf == 0)[0]\n",
    "\t\t\trandom_indices = torch.randperm(neg_indices.size(0))[:num_neg_needed]\n",
    "\t\t\tneg_indices_to_keep = neg_indices[random_indices]\n",
    "\t\t\tconf_loss_mask[neg_indices_to_keep] = 1\n",
    "\t\telse:\n",
    "\t\t\tconf_loss_mask = torch.ones_like(y_true_conf)\n",
    "\n",
    "\t\tyield images, y_true_conf, y_true_loc, conf_loss_mask"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RUN TRAINING FUNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    \"\"\"\n",
    "    Load training and test data\n",
    "    Run training process\n",
    "    Plot train/validation losses\n",
    "    Report test loss\n",
    "    Save model\n",
    "    \"\"\"\n",
    "    # Load training and test data\n",
    "    train_image_paths = train_image_files  # List of image file paths\n",
    "    train_annotation_paths = train_txt_files  # List of annotation file paths\n",
    "\n",
    "    test_image_paths = test_image_files  # List of image file paths\n",
    "    test_annotation_paths = test_txt_files  # List of annotation file paths\n",
    "    \n",
    "    transform = tf.keras.Sequential([\n",
    "        tf.keras.layers.experimental.preprocessing.Resizing(IMG_W, IMG_H),\n",
    "        tf.keras.layers.experimental.preprocessing.Rescaling(1./255),\n",
    "        # Add other transformations as needed\n",
    "    ])\n",
    "\n",
    "    train_dataset = CustomDataset(train_image_paths, train_annotation_paths, transform=transform)\n",
    "    X_train = train_dataset.image_paths\n",
    "    y_train_conf = train_dataset.annotation_paths\n",
    "    y_train_loc = train_dataset.annotation_paths\n",
    "\n",
    "    X_train = np.array(X_train)\n",
    "    y_train_conf = np.array(y_train_conf)\n",
    "    y_train_loc = np.array(y_train_loc)\n",
    "\n",
    "    test_dataset = CustomDataset(test_image_paths, test_annotation_paths, transform=transform)\n",
    "    X_valid = test_dataset.image_paths\n",
    "    y_valid_conf = test_dataset.annotation_paths\n",
    "    y_valid_loc = test_dataset.annotation_paths\n",
    "\n",
    "    X_valid = np.array(X_valid)\n",
    "    y_valid_conf = np.array(y_valid_conf)\n",
    "    y_valid_loc = np.array(y_valid_loc)\n",
    "    \n",
    "    # Instantiate neural network, get relevant tensors\n",
    "    model = SSDModel()\n",
    "    x = model['x']\n",
    "    y_true_conf = model['y_true_conf']\n",
    "    y_true_loc = model['y_true_loc']\n",
    "    conf_loss_mask = model['conf_loss_mask']\n",
    "    is_training = model['is_training']\n",
    "    optimizer = model['optimizer']\n",
    "    reported_loss = model['loss']\n",
    "\n",
    "    # Training process\n",
    "    # TF saver to save/restore trained model\n",
    "    saver = tf.train.Checkpoint(model=model)\n",
    "    manager = tf.train.CheckpointManager(saver, './model', max_to_keep=1)\n",
    "\n",
    "    if RESUME:\n",
    "        print('Restoring previously trained model')\n",
    "        checkpoint.restore(manager.latest_checkpoint)\n",
    "        \n",
    "        # Restore previous loss history\n",
    "        with open('loss_history.p', 'rb') as f:\n",
    "            loss_history = pickle.load(f)\n",
    "    else:\n",
    "        print('Training model from scratch')\n",
    "        # Variable initialization\n",
    "        model.initialize()\n",
    "\n",
    "        # For book-keeping, keep track of training and validation loss over epochs\n",
    "        loss_history = []\n",
    "\n",
    "    # Record time elapsed for performance check\n",
    "    last_time = time.time()\n",
    "    train_start_time = time.time()\n",
    "\n",
    "    # Run NUM_EPOCH epochs of training\n",
    "    for epoch in range(NUM_EPOCH):\n",
    "        train_gen = next_batch(X_train, y_train_conf, y_train_loc, BATCH_SIZE)\n",
    "        num_batches_train = math.ceil(X_train.shape[0] / BATCH_SIZE)\n",
    "        losses = []  # list of loss values for book-keeping\n",
    "\n",
    "        # Run training on each batch\n",
    "        for _ in range(num_batches_train):\n",
    "            # Obtain the training data and labels from generator\n",
    "            images, y_true_conf_gen, y_true_loc_gen, conf_loss_mask_gen = next(train_gen)\n",
    "\n",
    "            # Perform gradient update (i.e. training step) on current batch\n",
    "            with tf.GradientTape() as tape:\n",
    "                loss = model.compute_loss(images, y_true_conf_gen, y_true_loc_gen, conf_loss_mask_gen)\n",
    "            \n",
    "            gradients = tape.gradient(loss, model.trainable_variables)\n",
    "            optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "            \n",
    "            losses.append(loss)  # TODO: Need mAP metric instead of raw loss\n",
    "\n",
    "        # A rough estimate of loss for this epoch (overweights the last batch)\n",
    "        train_loss = np.mean(losses)\n",
    "\n",
    "        # Calculate validation loss at the end of the epoch\n",
    "        valid_gen = next_batch(X_valid, y_valid_conf, y_valid_loc, BATCH_SIZE)\n",
    "        num_batches_valid = math.ceil(X_valid.shape[0] / BATCH_SIZE)\n",
    "        losses = []\n",
    "        for _ in range(num_batches_valid):\n",
    "            images, y_true_conf_gen, y_true_loc_gen, conf_loss_mask_gen = next(valid_gen)\n",
    "\n",
    "            # Perform forward pass and calculate loss\n",
    "            loss = model.compute_loss(images, y_true_conf_gen, y_true_loc_gen, conf_loss_mask_gen)\n",
    "            losses.append(loss)\n",
    "        valid_loss = np.mean(losses)\n",
    "\n",
    "        # Record and report train/validation/test losses for this epoch\n",
    "        loss_history.append((train_loss, valid_loss))\n",
    "\n",
    "        # Print accuracy every epoch\n",
    "        print('Epoch %d -- Train loss: %.4f, Validation loss: %.4f, Elapsed time: %.2f sec' %\\\n",
    "            (epoch+1, train_loss, valid_loss, time.time() - last_time))\n",
    "        last_time = time.time()\n",
    "\n",
    "    total_time = time.time() - train_start_time\n",
    "    print('Total elapsed time: %d min %d sec' % (total_time/60, total_time%60))\n",
    "\n",
    "    test_loss = 0.  # TODO: Add test set\n",
    "    '''\n",
    "    # After training is complete, evaluate accuracy on test set\n",
    "    print('Calculating test accuracy...')\n",
    "    test_gen = next_batch(X_test, y_test, BATCH_SIZE)\n",
    "    test_size = X_test.shape[0]\n",
    "    test_acc = calculate_accuracy(test_gen, test_size, BATCH_SIZE, accuracy, x, y, keep_prob, sess)\n",
    "    print('Test acc.: %.4f' % (test_acc,))\n",
    "    '''\n",
    "\n",
    "    if SAVE_MODEL:\n",
    "        # Save model to disk\n",
    "        save_path = manager.save()\n",
    "        print('Trained model saved at: %s' % save_path)\n",
    "\n",
    "        # Also save accuracy history\n",
    "        print('Loss history saved at loss_history.p')\n",
    "        with open('loss_history.p', 'wb') as f:\n",
    "            pickle.dump(loss_history, f)\n",
    "\n",
    "    # Return final test accuracy and accuracy_history\n",
    "    return test_loss, loss_history"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 5152), dtype=tf.float32, name=None), name='flatten_1/Reshape:0', description=\"created by layer 'flatten_1'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 1056), dtype=tf.float32, name=None), name='flatten_3/Reshape:0', description=\"created by layer 'flatten_3'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 160), dtype=tf.float32, name=None), name='flatten_5/Reshape:0', description=\"created by layer 'flatten_5'\")\n",
      "KerasTensor(type_spec=TensorSpec(shape=(None, 6368), dtype=tf.float32, name=None), name='concatenate_1/concat:0', description=\"created by layer 'concatenate_1'\")\n",
      "31248\n",
      "(None, 6368)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling layer \"tf.math.subtract\" (type TFOpLambda).\n\nDimensions must be equal, but are 31248 and 6368 for '{{node tf.math.subtract/Sub}} = Sub[T=DT_FLOAT](Placeholder, Placeholder_1)' with input shapes: [?,31248], [?,6368].\n\nCall arguments received by layer \"tf.math.subtract\" (type TFOpLambda):\n  • x=tf.Tensor(shape=(None, 31248), dtype=float32)\n  • y=tf.Tensor(shape=(None, 6368), dtype=float32)\n  • name=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m__name__\u001b[39m \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39m__main__\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m \trun_training()\n",
      "Cell \u001b[1;32mIn[18], line 41\u001b[0m, in \u001b[0;36mrun_training\u001b[1;34m()\u001b[0m\n\u001b[0;32m     38\u001b[0m y_valid_loc \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(y_valid_loc)\n\u001b[0;32m     40\u001b[0m \u001b[39m# Instantiate neural network, get relevant tensors\u001b[39;00m\n\u001b[1;32m---> 41\u001b[0m model \u001b[39m=\u001b[39m SSDModel()\n\u001b[0;32m     42\u001b[0m x \u001b[39m=\u001b[39m model[\u001b[39m'\u001b[39m\u001b[39mx\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     43\u001b[0m y_true_conf \u001b[39m=\u001b[39m model[\u001b[39m'\u001b[39m\u001b[39my_true_conf\u001b[39m\u001b[39m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[15], line 11\u001b[0m, in \u001b[0;36mSSDModel\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m      9\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mNotImplementedError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mModel \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m not supported\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m MODEL)\n\u001b[1;32m---> 11\u001b[0m model_helper \u001b[39m=\u001b[39m ModelHelper(model[\u001b[39m'\u001b[39;49m\u001b[39my_pred_conf\u001b[39;49m\u001b[39m'\u001b[39;49m], model[\u001b[39m'\u001b[39;49m\u001b[39my_pred_loc\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     13\u001b[0m ssd_model \u001b[39m=\u001b[39m {}\n\u001b[0;32m     14\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mkeys():\n",
      "Cell \u001b[1;32mIn[13], line 37\u001b[0m, in \u001b[0;36mModelHelper\u001b[1;34m(y_pred_conf, y_pred_loc)\u001b[0m\n\u001b[0;32m     34\u001b[0m conf_loss \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mreduce_sum(conf_loss)\n\u001b[0;32m     36\u001b[0m \u001b[39m# Localization loss (smooth L1 loss)\u001b[39;00m\n\u001b[1;32m---> 37\u001b[0m diff \u001b[39m=\u001b[39m y_true_loc \u001b[39m-\u001b[39;49m y_pred_loc\n\u001b[0;32m     39\u001b[0m loc_loss_l2 \u001b[39m=\u001b[39m \u001b[39m0.5\u001b[39m \u001b[39m*\u001b[39m (diff \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m \u001b[39m2.0\u001b[39m)\n\u001b[0;32m     40\u001b[0m loc_loss_l1 \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mabs(diff) \u001b[39m-\u001b[39m \u001b[39m0.5\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Clarissa Angelia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m--> 153\u001b[0m   \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    154\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    155\u001b[0m   \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Clarissa Angelia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\layers\\core\\tf_op_layer.py:119\u001b[0m, in \u001b[0;36mKerasOpDispatcher.handle\u001b[1;34m(self, op, args, kwargs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Handle the specified operation with the specified arguments.\"\"\"\u001b[39;00m\n\u001b[0;32m    115\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(\n\u001b[0;32m    116\u001b[0m     \u001b[39misinstance\u001b[39m(x, keras_tensor\u001b[39m.\u001b[39mKerasTensor)\n\u001b[0;32m    117\u001b[0m     \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m tf\u001b[39m.\u001b[39mnest\u001b[39m.\u001b[39mflatten([args, kwargs])\n\u001b[0;32m    118\u001b[0m ):\n\u001b[1;32m--> 119\u001b[0m     \u001b[39mreturn\u001b[39;00m TFOpLambda(op)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    120\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mNOT_SUPPORTED\n",
      "File \u001b[1;32mc:\\Users\\Clarissa Angelia\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling layer \"tf.math.subtract\" (type TFOpLambda).\n\nDimensions must be equal, but are 31248 and 6368 for '{{node tf.math.subtract/Sub}} = Sub[T=DT_FLOAT](Placeholder, Placeholder_1)' with input shapes: [?,31248], [?,6368].\n\nCall arguments received by layer \"tf.math.subtract\" (type TFOpLambda):\n  • x=tf.Tensor(shape=(None, 31248), dtype=float32)\n  • y=tf.Tensor(shape=(None, 6368), dtype=float32)\n  • name=None"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\trun_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
